##### 简单自编码器

+ 自编码器原理

  > 尝试利用数据$x$本身作为监督信号指导网络训练，即学习映射如下：

  $$
  f_{\theta}:x \rightarrow x \tag{1}
  $$

  > 以上网络分为两个部分，分别为编码层和解码层，分别如下：

  $$
  g_{\theta_1}: x \rightarrow z \tag{2}
  $$

  $$
  h_{\theta_2} = z \rightarrow x \tag{3}
  $$

  > 以上两个子网络分别被称为编码器Encoder和解码器Decoder，整个网络的优化目标可表示如下：

  $$
  \min L = \operatorname{dist}(x, \bar{x}) \tag{4}
  $$
  
  > 其中：
  
  $$
  \bar{x} = h_{\theta_2}(g_{\theta_1}(x)) \tag{5}
  $$

  > 通常，选取欧氏距离作为误差函数，即：

  $$
  L = \sum_i(x_i - \bar{x}_i)^2 \tag{6}
  $$
  
  > 相对于PCA等线性方法，自编码器性能更加优秀，甚至可以更加完美的恢复输出。

---

##### 自编码器变种

+ ​	Denoising Auto-Encoder

  > 去噪自编码器，主要目的为防止神经网络记忆输入数据的底层特征，通过给输入数据添加随机的噪声扰动，例如可以添加高斯分布噪声如下：

  $$
  \tilde{x} = x + \epsilon,\epsilon \sim N(0, var) \tag{7}
  $$

  > 则自编码器的参数优化目标为：

  $$
  \theta^{\*} = \underbrace{\operatorname{argmin}}_{\theta}\operatorname{dist}(h_{\theta_2}(g_{\theta_1}(x)), x) \tag{8}
  $$

+ Dropout Auto-Encoder

  > 自编码器面临过拟合风险，除对输入添加随机噪声外，还可以将dropout机制引入，在网络层中插入dropout层即可实现网络连接的随机断开。

+ Adversarial Auto-Encoder

  > 对抗自编码器，利用额外的判别器网络判定降维的隐藏向量$z$是否采样自先验分布$p(z)$。通过该训练方法，除了可以重建样本，还可以约束条件概率$q(z|x)$趋近先验分布$p(z)$。