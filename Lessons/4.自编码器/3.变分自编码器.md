##### 变分自编码器

> 传统自编码器本质上是学习输入$x$和隐藏变量$z$之间的映射关系，属于判别模型。若尝试将自编码器调整为生成模型，则需要引出变分自编码器。

> 给定隐藏变量的分布$P(z)$，如果可以学习到条件概率分布$P(x|z)$，则可以通过对联合概率分布$P(x, z) = P(x|z)P(z)$进行采样，生成不同的样本。

> 从神经网络角度来看，VAE同样具有编码器和解码器两个子网络。编码器接受输入$x$，输出为隐变量$z$；解码器负责将隐变量$z$解码为重建的$\bar{x}$。另一方面，VAE模型对隐变量$z$的分布有显式约束，希望隐变量$z$符合预设的先验分布$P(z)$。因此在损失函数的设计上，除了原有的重建误差之外，还添加了隐变量$z$分布的约束项。

+ VAE原理

  > 假设任何数据集采样自某个分布$p(x|z)$，$z$是隐藏变量，代表内部特征，例如手写数字图片$x$，$z$可以表示字体的大小、书写风格等特性，并符合某个先验分布$p(z)$，在给定具体隐藏变量$z$的情况下，可以从分布$p(x|z)$中采样一系列生成样本。

  > 假设$p(z)$符合已知分布，例如$N(0,1)$，在已知该分布的条件下，希望学会生成概率模型$p(x|z)$，可以采用最大似然方法，即神经网络的优化目标变为：

  $$
  \underset{\theta}{\max}p(x) = \int_zp(x|z)p(z)dz \tag{1}
  $$

  > 由于$z$为连续变量，上述积分无法转换为离散形式，导致很难直接优化。因此，利用变分推断思想，通过分布$q_{\phi}(z|x)$逼近$p(z|x)$，即最小化这两个分布之间的距离，利用KL散度衡量如下：

  $$
  \underset{\phi}{\min}D_{KL}(q_{\phi}(z|x)||p(z|x)) \tag{2}
  $$

  > 代入KL散度公式后如下：

  $$
  \begin{aligned}
  D_{KL}(q_{\phi}(z|x)||p(z|x)) 
  &= \int_zq_{\phi}(z|x)\log\frac{q_{\phi}(z|x)}{p(z|x)}dz \\\\
  &= \int_zq_{\phi}(z|x)\log\frac{q_{\phi}(z|x)p(x)}{p(x,z)}dz \\\\
  &= \int_zq_{\phi}(z|x)\log\frac{q_{\phi}(z|x)}{p(x,z)}dz+\int_zq_{\phi}(z|x)\log p(x)dz \\\\
  &= -\underbrace{(-\int_zq_{\phi}(z|x)\log\frac{q_{\phi}(z|x)}{p(x,z)}dz)}_{L(\phi,\theta)} + \log p(x)
  \end{aligned} \tag{3}
  $$

  > 考虑到散度大于等于0，因此结合公式(3)，有：

  $$
  L(\phi,\theta) \le \log p(x) \tag{4}
  $$

  > 结合公式(4)，$L(\phi,\theta)$是$\log p(x)$的下界限。由于目标为最大化似然概率$p(x)$，因此可以通过最大化其下界限$L(\phi,\theta)$来实现。

  $$
  \begin{aligned}
  L(\phi,\theta) 
  &= \int_zq_{\phi}(z|x)\log\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}dz \\\\
  &= \int_zq_{\phi}(z|x)\log\frac{p(z)p_{\theta}(x|z)}{q_{\phi}(z|x)}dz \\\\
  &= \int_zq_{\phi}(z|x)\log\frac{p(z)}{q_{\phi}(z|x)}dz+\int_zq_{\phi}(z|x)\log p_{\theta}(x|z)dz \\\\
  &= -D_{KL}(q_{\phi}(z|x)||p(z))+E_{z\sim q}[\log p_{\theta}(x|z)]
  \end{aligned} \tag{5}
  $$

  > 根据上式，可以用编码器网络$q_{\phi}(z|x)$函数和解码器网络$p_{\theta}(x|z)$函数进行计算，优化目标函数。

  > 当$q_{\phi}(z|x)$为正态分布$N(\mu_1, \sigma_1)$，$p(z)$为正态分布$N(0, 1)$时，$D_{KL}(q_{\phi}(z|x)||p(z))$可进一步化简为：

  $$
  \begin{aligned}
  D_{KL}(q_{\phi}(z|x)||p(z)) 
  &= \log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{\sigma_2^2}-\frac{1}{2}\\\\
  &= -\log\sigma_1+0.5\sigma_1^2+0.5\mu_1^2-0.5
  \end{aligned} \tag{6}
  $$

  > 上式中，散度项可以简单计算，期望项也可以基于自编码器中的重建误差函数实现。

  > 经过以上推理，VAE模型的优化目标由最大化$L(\phi, \theta)$转换为$\min D_{KL}(q_{\phi}(z|x)||p(z))$和$\max E_{z\sim q}\log p_{\theta}(x|z)$；第一项优化目标可以理解为约束隐变量$z$的分布，第二项优化目标可以理解为提高网络的重建效果。
  
+ 梯度可导性

  > 当$q_{\phi}(z|x)$和$p(z)$均为正态分布时，编码器输出正态分布的均值$\mu$和方差$\sigma^2$，解码器的输入采样自$N(\mu,\sigma^2)$，由于采样操作导致梯度不连续，因此需要连续可导方案如下：

  $$
  z = \mu + \sigma \odot\epsilon,\epsilon\sim N(0,1) \tag{7}
  $$

  > 通过如上操作可以保证其梯度反向传播的连续性，网络训练过程如下：输入$x$通过编码器网络$q_{\phi}(z|x)$计算得到隐变量$z$的均值与方差，通过以上采样方式获得隐变量$z$，并送入解码器网络，获得分布$p_{\theta}(x|z)$，计算误差并优化函数。

---

##### KL散度推导

+ 分布
  $$
  N(x|\mu, \Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}
  $$

+ 性质
  $$
  E(xx^T) = \Sigma + \mu\mu^T
  $$

  $$
  E(x^TAx) = tr(A\Sigma) + \mu^TA\mu
  $$

+ 推导
  $$
  \begin{aligned}
  D_{KL}{(p||q)} 
  &= E_p[\log \frac{p}{q}] \\\\
  &= E_p[\log p - \log q] \\\\
  &= E_p[\log\frac{1}{|\Sigma_1|^{1/2}}-\frac{1}{2}(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1)-\log\frac{1}{|\Sigma_2|^{1/2}}+\frac{1}{2}(x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2)] \\\\
  &= \frac{1}{2}E_p[-\log|\Sigma_1|-(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1)+\log|\Sigma_2|+(x-\mu_2)\Sigma_2^{-1}(x-\mu_2)] \\\\
  &= \frac{1}{2}\log\frac{|\Sigma_2|}{|\Sigma_1|}+\frac{1}{2}E_p[-(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1)+(x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2)] \\\\
  &= \frac{1}{2}\log\frac{|\Sigma_2|}{|\Sigma_1|}+\frac{1}{2}E_p\lbrace-tr[\Sigma_1^{-1}(x-\mu_1)(x-\mu_1)^T]+tr[\Sigma_2^{-1}(x-\mu_2)(x-\mu_2)^T]\rbrace \\\\
  &= \frac{1}{2}\log\frac{|\Sigma_2|}{|\Sigma_1|}-\frac{1}{2}tr\lbrace\Sigma_1^{-1}E_p[(x-\mu_1)(x-\mu_1)^T]\rbrace+\frac{1}{2}tr\lbrace\Sigma_2^{-1}E_p[(x-\mu_2)(x-\mu_2)^T]\rbrace \\\\
  &= \frac{1}{2}\log\frac{|\Sigma_2|}{|\Sigma_1|}-\frac{1}{2}tr\lbrace\Sigma_1^{-1}\Sigma_1\rbrace+\frac{1}{2}tr\lbrace\Sigma_2^{-1}E_p[xx^T-\mu_2x^T-x\mu_2^T+\mu_2\mu_2^T]\rbrace \\\\
  &= \frac{1}{2}\log\frac{|\Sigma_2|}{|\Sigma_1|}-\frac{1}{2}n+\frac{1}{2}tr\lbrace\Sigma_2^{-1}[\Sigma_1+\mu_1\mu_1^T-\mu_2\mu_1^T-\mu_1\mu_2^T+\mu_2\mu_2^T]\rbrace \\\\
  &= \frac{1}{2}\lbrace\log\frac{|\Sigma_2|}{|\Sigma_1|}-n+tr(\Sigma_2^{-1}\Sigma_1)+tr[\Sigma_2^{-1}(\mu_1\mu_1^T-\mu_2\mu_1^T-\mu_1\mu_2^T+\mu_2\mu_2^T)]\rbrace \\\\
  &= \frac{1}{2}\lbrace\log\frac{|\Sigma_2|}{|\Sigma_1|}-n+tr(\Sigma_2^{-1}\Sigma_1)+tr(\mu_1^T\Sigma_2^{-1}\mu_1-2\mu_1^T\Sigma_2^{-1}\mu_2+\mu_2^T\Sigma_2^{-1}\mu_2)\rbrace \\\\
  &= \frac{1}{2}\lbrace\log\frac{|\Sigma_2|}{|\Sigma_1|}-n+tr(\Sigma_2^{-1}\Sigma_1)+(\mu_2-\mu_1)^T\Sigma_2^{-1}(\mu_2-\mu_1)\rbrace
  \end{aligned}
  $$

  > 当为单变量高斯分布时，若有如下：

  $$
  p\sim N(\mu_1, \sigma_1),q\sim N(\mu_2, \sigma_2)
  $$

  > 代入推导式中，有：

  $$
  D_{KL}(p||q) = \log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{\sigma_2^2}-\frac{1}{2}
  $$

  































