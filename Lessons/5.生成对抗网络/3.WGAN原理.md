##### WGAN原理

+ JS散度缺陷

  > 考虑完全不重叠$(\theta \ne 0)$的两个分布$p$和$q$，其中分布$p$为：

  $$
  \forall(x,y)\in p,x=0,y\sim U(0,1) \tag{1}
  $$

  > 分布$q$为：

  $$
  \forall(x,y)\in q,x=\theta,y\sim U(0,1) \tag{2}
  $$

  > 分析上述分布$p$和$q$之间的JS散度随$\theta$的变化情况，当$\theta=0$时，有：

  $$
  D_{JS}(p||q) = \frac{1}{2}(\sum_{x=0,y\sim U(0,1)}1 \cdot \log\frac{1}{1}+\sum_{x=0,y\sim U(0,1)}1 \cdot \log\frac{1}{1}) = 0 \tag{3}
  $$

  > 同理，当$\theta\ne 0$，有：

  $$
  D_{JS}(p||q) = \log2 \tag{4}
  $$

  > 根据以上结果，当两个分布完全不重叠时，无论分布之间的举例远近，JS散度为恒定值$\log2$，此时JS梯度无法产生有效的梯度信息；当两个分布出现重叠时，JS散度平滑变动，产生有效梯度信息；完全重合后，JS梯度取得最小值0。因此，在网络初始训练时，由于两个分布没有重叠，因此生成样本位置处的梯度始终为0，无法更新生成网络的参数，从而出现网络训练困难的现象。

+ EM距离

  > Wasserstein距离，推土机距离，表示一个分布变换到另一个分布的最小代价，定义为：

  $$
  W(p, q) = \underset{\gamma \sim \prod(p, q)}{\inf}E_{(x, y) \sim \gamma}[||x-y||] \tag{5}
  $$

  > 其中，$\prod(p, q)$是分布$p$和$q$组合起来的所有可能的联合分布的集合，对于每个可能的联合分布$\gamma \sim \prod(p, q)$，计算$||x - y||$的期望，其中$(x, y)$采样自联合分布$\gamma$。不同的联合分布$\gamma$有不同的期望$E_{(x,y) \sim \gamma}[||x-y||]$，这些期望中的下确界定义为分布$p$和$q$的Wasserstein举例，其中$\inf\lbrace\cdot\rbrace$表示集合的下确界。
  
+ WGAN-GP

  > 考虑到不可能遍历所有的联合分布$\gamma$去计算举例$||x-y||$的期望，因此需要给予Kantorovich-Rubinstein对偶性将直接求$W(p_r,p_g)$转换为如下：

  $$
  W(p_r, p_g) = \frac{1}{K}\underset{||f||_L \le K}{\sup}(E_{x \sim p_r}[f(x)]-E_{x \sim p_g}[f(x)]) \tag{6}
  $$

  > 其中，$\sup\lbrace\cdot\rbrace$表示集合的上确界，$||f||_L\le K$表示函数$f:R\rightarrow R$满足$K$阶Lipschitz连续性，即满足：

  $$
  |f(x_1) - f(x_2)| \le K \cdot |x_1 - x_2| \tag{7}
  $$

  > 于是，使用判别网络$D_{\theta}$参数化$f(x)$函数，并另$K=1$，此时公式(6)可变为：

  $$
  W(p_r, p_g) = \underset{||D_{\theta}||_L \le 1}{\sup}(E_{x \sim p_r}[D_{\theta}(x)] - E_{x \sim p_g}[D_{\theta}(x)]) \tag{8}
  $$

  > 因此，上述问题转化为：

  $$
  \underset{\theta}{\max}\lbrace E_{x \sim p_r}[D_{\theta}(x)] - E_{x \sim p_g}[D_{\theta}(x)]\rbrace \tag{9}
  $$

  > 上式即为判别器D的优化目标，且需要满足1阶Lipschitz约束。采用梯度惩罚项方法可以迫使判别网络满足该约束，其定义为：

  $$
  GP = E_{\hat{x} \sim p_{\hat{x}}}[(||\nabla_{\hat{x}}D(\hat{x})||_2 - 1)^2] \tag{10}
  $$

  > 因此，最终WGAN的判别器D的训练目标变为：

  $$
  \underset{\theta}{\max}L(G, D) = \underbrace{E_{x_r \sim p_r}[D(x_r)] - E_{x_f \sim p_g}[D(x_f)]}_{EM} - \underbrace{\lambda E_{\hat{x} \sim p_{\hat{x}}}[(||\nabla_{\hat{x}}D(\hat{x})||_2 - 1)^2]}_{GP} \tag{11}
  $$

  > 其中，$\hat{x}$来自$x_r$与$x_f$的线性插值：

  $$
  \hat{x} = tx_r + (1 - t)x_f,t \in [0, 1] \tag{12}
  $$

  > 因此，判别器的目标为最大化以上损失函数，即使生成器G的分布$p_g$与真实分布$p_r$之间的EM举例尽可能大，且梯度项趋近于1。

  > WGAN的生成器G的训练目标为最小化以下损失函数：

  $$
  \underset{\phi}{\min}L(G, D) = \underbrace{E_{x_r \sim p_r}[D(x_r)] - E_{x_f \sim p_g}[D(x_f)]}_{EM} \tag{13}
  $$

  > 即生成器的分布$p_g$与真实分布$p_r$之间的EM距离越小越好，排除无关项，损失函数可简化如下：

  $$
  \underset{\phi}{\min}L(G, D) = -E_{x_f \sim p_g}[D(x_f)] = -E_{z \sim p_z(\cdot)}[D(G(z))] \tag{14}
  $$

  > 判别网络D输出不需要添加Sigmoid激活函数，因为原始版本的判别器的功能为二分类网络，而WGAN中判别器作为EM距离的度量网络，属于实数空间。在训练WGAN时，推荐使用RMSProp或SGD等不带动量的优化器。