##### GAN网络

+ 网络结构

  > 生成对抗网络分为两个子网络：生成网络和判别网络，其中生成网络G负责学习样本的真实分布，判别网络D负责将生成网络采样的样本与真实样本区分开来。

  > 生成网络$G(z)$从先验分布$p_z(\cdot)$中采样隐藏变量$z$，通过生成网络G参数化的$p_g(x|z)$分布，获取生成样本$x$，其中隐藏变量$z$的先验分布可以假设为某种已知的分布。$p_g(x|z)$可以用深度神经网络参数化，从输入输出层面来看，生成器G的功能是将隐变量$z$通过神经网络转换成样本向量$x_f$，下标$f$表示该样本为假样本。

  > 判别网络$D(x)$与普通二分类网络功能类似，接受输入样本$x$的数据集，包含了采样自真实数据分布$p_r(\cdot)$的样本$x_r$，也包含了来自生成网络的假样本$x_f$，由两者共同组成判别网络的训练集，真实样本标签为1，假样本标签为0，通过最小化判别网络D的预测值与标签之间的误差优化判别网络参数。

+ 网络训练

  > 对于判别网络D，目标是区分真假样本。以图片生成为例，目标为最小化图片的预测值和真实值之间的交叉熵函数：

  $$
  \underset{\theta}{\min}L = CE(D_{\theta}(x_r), y_r, D_{\theta}(x_f), y_f) \tag{1}
  $$

  > 其中，$CE$表示交叉熵函数，二分类问题中可表示为：

  $$
  L = -\sum_{x_r\sim p_r(\cdot)}\log D_{\theta}(x_r)-\sum_{x_f\sim p_g(\cdot)}\log (1-D_{\theta}(x_f)) \tag{2}
  $$

  > 因此，判别网络D的优化目标为：

  $$
  \begin{aligned}
  \theta^{\*} 
  &= \underset{\theta}{\operatorname{argmin}}\lbrace-\sum_{x_r\sim p_r(\cdot)}\log D_{\theta}(x_r)-\sum_{x_f\sim p_g(\cdot)}\log (1-D_{\theta}(x_f))\rbrace \\\\
  &= \underset{\theta}{\operatorname{argmax}}\lbrace E_{x_r\sim p_r(\cdot)}\log D_{\theta}(x_r) + E_{x_f\sim p_g(\cdot)}\log(1-D_{\theta}(x_f))\rbrace
  \end{aligned} \tag{3}
  $$

  > 对于生成网络G，希望$x_f=G(z)$能够骗过判别网络，假样本$x_f$在判别网络的输出越接近真实标签越好，即在训练生成网络时，希望判别网络的输出$D(G(z))$逼近1，即最小化其与1之间的交叉熵损失函数如下：

  $$
  \underset{\phi}{\min}L=CE(D(G_{\phi}(z)), 1) = -\log D(G_{\phi}(z)) \tag{4}
  $$

  > 对照公式(3)，上式可改写为：

  $$
  \phi^{\*} = \underset{\phi}{\operatorname{argmin}}L=E_{z\sim p_z(\cdot)}\log(1-D(G_{\phi}(z))) \tag{5}
  $$

  > 将判别网络和生成网络的目标合并，算法流程可表示如下：

  > > 随机初始化参数$\theta$和$\phi$
  >
  > > $repeat$
  > >
  > > > 循环：判别器$k$次迭代更新
  > > >
  > > > > 随机采样隐向量$z\sim p_z(\cdot)$
  > > >
  > > > > 随机采样真实样本$x_r\sim p_r(\cdot)$
  > > >
  > > > > 根据梯度上升算法更新$D$网络
  > > > >
  > > > > > $\nabla_{\theta}E_{x_r\sim p_r(\cdot)}\log D_{\theta}(x_r) + E_{x_f\sim p_g(\cdot)}\log(1-D_{\theta}(x_f))$
  > >
  > > > 随机采样隐向量$z\sim p_z(\cdot)$
  > >
  > > > 根据梯度下降算法更新$G$网络
  > > >
  > > > > $\nabla_{\phi}E_{z\sim p_z(\cdot)}\log(1-D(G_{\phi}(z)))$
  > >
  > > $until$ 训练回合达到要求
  >
  > > 输出训练好的生成器$G_{\phi}$

---

##### GAN变种

+ DCGAN

  > 使用转置卷积层代替全连接层(VAE)实现生成网络，普通卷积层实现判别网络，降低网络参数量，同时图片的生成效果大幅提升。

+ InfoGAN

  > 尝试使用无监督模式学习输入$x$的可解释向量$z$的表示方法，即希望隐向量$z$能够对应到数据的语义特征，从而可以通过认为控制隐变量参数得到想要的结果。将特征进行分离后，神经网络的可解释性更强，用户可以自由定义目标输出的生成。

+ CycleGAN

  > 无监督方式进行图片风格相互转换算法，其基本假设为图片A转换到图片B，再从图片B转换到图片A'，那么A'与A应该是同一张图片。在该网络中，除了设计标准的GAN损失项外，还增设了循环一致性损失保证两者逼近。

+ WGAN

  > GAN训练容易出现训练不收敛和模式崩塌现象，WGAN从理论层面分析了原始GAN使用
  > JS散度存在的缺陷，提出使用Wasserstein距离解决该问题，通过添加梯度惩罚项，提高了网络训练的稳定性。

+ Equal GAN

  > GAN网络变种基本能达到想死的性能，以上模型均不包含本质上的创新。

+ Self-Attention GAN

  > 加入自注意力机制，具体可参考自然语言处理部分内容。

+ BigGAN

  > 尝试将训练扩展到大规模，利用正交正则化等技巧保证训练过程的稳定性。

---

##### 纳什均衡

+ 判别器状态

  > 回顾判别器损失函数，如下：

  $$
  \begin{aligned}
  L(G, D) 
  &= \int_x p_r(x)\log(D(x))dx+\int_z p_z(z)\log(1-D(G(z)))dz \\\\
  &= \int_x \lbrace p_r(x)\log(D(x))+p_g(x)\log(1-D(x))\rbrace dx
  \end{aligned} \tag{6}
  $$

  > 对于判别器而言，优化目标是最大化$L(G,D)$，需要找到内部函数的最大值，考虑通用情况如下：

  $$
  f(x) = A\log x+B\log(1-x) \tag{7}
  $$

  > 对公式(7)求导，如下：

  $$
  \frac{df(x)}{dx} =A\frac{1}{\ln10}\frac{1}{x}-B\frac{1}{\ln10}\frac{1}{1-x}=\frac{1}{\ln10}\frac{A-(A+B)x}{x(1-x)}\tag{8}
  $$

  > 可以求得$f(x)$函数的极值点：

  $$
  x = \frac{A}{A+B} \tag{9}
  $$

  > 代入公式(6)的求导结果，则有：

  $$
  D_{\theta} = \frac{p_r(x)}{p_r(x)+p_g(x)} \tag{10}
  $$

  > 即，当判别器网络处于以上值时，函数$f_{\theta}$取得最大值，此时损失函数$L(G,D)$也取得最大值。

+ 生成器状态

  > 首先介绍与$KL$散度类似的另一个分布举例度量标准：$JS$散度，其分别定义为如下：

  $$
  D_{KL}(p||q) = \int_xp(x)\log\frac{p(x)}{q(x)}dx \tag{11}
  $$

  $$
  D_{JS}(p||q) = \frac{1}{2}D_{KL}(p||\frac{p+q}{2})+\frac{1}{2}D_{KL}(q||\frac{p+q}{2}) \tag{12}
  $$

  > 可以看到，$JS$散度客服了$KL$散度不对称缺陷，将上式展开：

  $$
  \begin{aligned}
  D_{JS}(p_r||p_g)
  &= \frac{1}{2}(\log2+\int_xp_r(x)\log\frac{p_r(x)}{p_r(x)+p_g(x)}dx)+\frac{1}{2}(\log2+\int_xp_g(x)\log\frac{p_g(x)}{p_r(x)+p_g(x)}dx) \\\\
  &= \log2+\frac{1}{2}(\int_xp_r(x)\log\frac{p_r(x)}{p_r(x)+p_g(x)}dx+\int_xp_g(x)\log\frac{p_g(x)}{p_r(x)+g_g(x)}dx)
  \end{aligned} \tag{13}
  $$

  > 结合公式(10)、公式(6)和公式(13)，可以发现：

  $$
  L(G, D^{\*}) = 2D_{JS}(p_r||p_g)-2\log2 \tag{14}
  $$

  > 对于生成网络而言，训练目标为最小化损失函数，考虑到$JS$散度恒大于等于0，因此仅当其为0时取得最小值，此时有：

  $$
  p_g=p_r \tag{15}
  $$

  > 因此，当学到的分布$p_g$与真实分布$p_r$一致时，网络达到平衡点，此时$D=0.5$，即此时生成的样本与真实样本来自同一分布，真假难辨，在判别器中均有相同的概率判定为真或假。

---

##### GAN训练难题

+ 超参数敏感

  > 网络结构设定、学习率、初始化状态等超参数对网络的训练过程影响较大，微量超参数调整将导致网络训练结果截然不同。

  > 一般可采用不使用Pooling层、多使用BN层、不使用全连接层、生成网络中使用relu激活函数、最后一层使用tanh激活函数、判别网络激活函数使用leaky_relu等。

+ 模式崩塌

  > 指模型生成的样本单一，多样性差。源于判别器仅鉴别单个样本是否采样自真实分布，无样本多样性显式约束，导致模型倾向于生成真实分布部分区间中的少量高质量样本。

  > 解决方案：WGAN