###### 卷积神经网络

+ 全连接网络局限

  > 以如下4个网络层的全连接网络为例：

  ```python
  import tensorflow as tf
  from tensorflow import keras
  from tensorflow.keras import layers, optimizers, losses, datasets, Sequential
  
  model = Sequential([
      layers.Dense(256, activation='relu'),
      layers.Dense(256, activation='relu'),
      layers.Dense(256, activation='relu')k
      layers.Dense(10)
  ])
  
  model.build(input_shape=(4, 784))
  model.summary()
  ```

  > 通过如上代码，可打印出模型每一层的参数量统计结果，如下表所示。

  |  层数  | 隐藏层1 | 隐藏层2 | 隐藏层3 | 输出层 |
  | :----: | :-----: | :-----: | :-----: | :----: |
  | 参数量 | 200960  |  65792  |  65792  |  2570  |

  > 该网络模型在训练时占用大量GPU空间，此结果表示：全连接层内存占用严重限制了神经网络朝着更大规模、更深层数方向的发展。由此引入了卷积神经网络的概念。

+ 卷积神经网络特性

  + 局部相关性

    > 摒弃稠密连接，基于距离的重要性分布假设特性即局部相关性应被应用于图片类二维信号的网络训练中。在该假设下，感受野在原始信号上进行训练，只关注和自己距离较近的部分节点，忽略较远的节点。

  + 权值共享

    > 根据局部相关性，每个输出节点仅与感受野内的输入节点相连接；另一方面，需要输出节点个数的感受野参数大小完成计算，因此通过权值共享的思想，对于每个输出节点$o_j$，均使用相同的权值矩阵$W$，则可以将参数量进一步减少。
  
+ 卷积运算

  > 在信号处理领域，1D连续卷积定义为：

  $$
  (f \otimes g)(n) = \int_{-\infty}^{\infty}f(\tau)g(n - \tau) d \tau \tag{1}
  $$

  > 对于离散卷积，转换成累加运算即可：

  $$
  (f \otimes g)(n) = \sum_{\tau = -\infty}^{\infty}f(\tau)g(n - \tau) \tag{2}
  $$

  > 在计算机视觉中，卷积运算基于2D图片函数$f(m, n)$和2D卷积核$g(m, n)$，其中$f(i, j)$和$g(i, j)$仅在各自窗口有效区域存在值，其他区域均为0，其定义为：

  $$
  [f \otimes g](m, n) = \sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty}f(i, j)g(m - i, n - j) \tag{3}
  $$

  > 计算举例如下：

  $$
  f(i, j) =
  \begin{bmatrix}
  2 & 3 & 1 \\\\
  0 & 5 & 1 \\\\
  1 & 0 & 8
  \end{bmatrix},
  g(i, j) = 
  \begin{bmatrix}
  0 & -1 & 0 \\\\
  -1 & 5 & -1 \\\\
  0 & -1 & 0
  \end{bmatrix} \tag{4}
  $$

  > 以其中某个输出计算过程举例如下：

  $$
  \begin{aligned}
  {[f \otimes g]}(-1, -1)
  &= \sum_{i \in [-1, 1]}^{\infty}\sum_{j \in [-1, 1]}^{\infty}f(i, j)g(-1 - i, -1 - j) \\\\
  &= (2 * 5) + (3 * (-1)) + (0 * (-1)) + (5 * 0) = 7
  \end{aligned} \tag{5}
  $$

  > 以上为从数学角度理解，卷积神经网络完成了2D函数的离散卷积运算；另外，亦可以从局部相关和权值共享角度理解，即卷积核在输入的特征图上进行逐步移动病完成权值相乘累加的运算。

+ 常见卷积核及效果

  > 原图

  $$
  g = \begin{bmatrix} 0 & 0 &0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \end{bmatrix} \tag{6}
  $$

  > 锐化

  $$
  g = \begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \end{bmatrix} \tag{7}
  $$

  > 模糊效果

  $$
  g = \begin{bmatrix} 0.0625 & 0.125 & 0.0625 \\\\ 0.125 & 0.25 & 0.125 \\\\ 0.0625 & 0.125 & 0.0625 \end{bmatrix} \tag{8}
  $$

  > 边缘提取效果

  $$
  g = \begin{bmatrix} -1 & -1 & -1 \\\\ -1 & 8 & -1 \\\\ -1 & -1 & -1 \end{bmatrix} \tag{9}
  $$

+ 卷积层计算相关参数

  + 卷积层输出尺寸$[b, h', w', c_out]$

    > 由卷积核的数量$c_out$，卷积核大小$k$，步长$s$，填充数$p$共同决定，公式如下：

    $$
    h' = [\frac{h + 2 * p_h - k}{s}] + 1 \tag{10}
    $$

    $$
    w' = [\frac{w + 2 * p_w - k}{s}] + 1 \tag{11}
    $$

    > 在TensorFlow中，当$s=1$时，若希望输出和输入等大小，则可设置参数padding='SAME'即可；当$s > 1$时，若保持该设置，将使得输出的维度成$\frac{1}{s}
    > $减少。

  + 卷积层实现方式

    > 主要分为函数和类两种实现方式，代码如下：

    ```python
    # function
    x = tf.random.normal([2, 5, 5, 3])
    w = tf.random.normal([3, 3, 3, 4])
    out = tf.nn.conv2d(x, w, strides=1, padding='SAME')
    
    # class
    x = tf.random.normal([2, 5, 5, 3])
    layer = layers.Conv2D(4, kernel_size=3, strides=1, padding='SAME')
    out = layer(x)
    ```

+ 梯度传播

  + DNN反向传播算法

    > 在深度神经网络中，输出层的误差$\delta^L$可表示为：

    $$
    \delta^L = \frac{\partial J(W, b)}{\partial z^L} = \frac{\partial J(W, b)}{a^L} \odot \sigma'(z^L) \tag{12}
    $$

    > 利用数学归纳法，用$\delta^{l + 1}$的值向前求出第$l$层的$\delta^l$，表达式如下：

    $$
    \begin{aligned}
    \delta^l 
    &= \frac{\partial J(W, b)}{\partial z^l} = \frac{\partial J(W, b)}{\partial z^{l + 1}} \cdot \frac{\partial z^{l + 1}}{\partial z^l} = \delta^{l + 1} \frac{\partial z^{l + 1}}{\partial z^l} \\\\
    &= (W^{l + 1})^T \delta^{l + 1} \odot \sigma'(z^l)
    \end{aligned} \tag{13}
    $$

    > 根据上式，则损失函数对$W$的梯度为：

    $$
    \frac{\partial J(W, b)}{\partial W^l} = \frac{\partial J(W, b)}{\partial z^l} \cdot \frac{\partial z^l}{\partial W^l} = \delta^l (a^{l - 1})^T \tag{14}
    $$

    > 同理，损失函数对$b$的梯度为：

    $$
    \frac{\partial J(W, b)}{\partial b^l} = \frac{\partial J(W, b)}{\partial z^l} \frac{\partial z^l}{\partial b^l} = \delta^l \tag{15}
    $$

  + CNN反向传播算法
  
    + 已知池化层$\delta^l$，推导上一隐藏层$\delta^{l - 1}$
  
      > 在前向传播算法中，池化层一般会采用Max或Average对输入进行池化。在反向传播时，首先需要将$\delta_l$的所有子矩阵大小还原到池化前的大小。若采用Average池化，则把$\delta^l$的所有子矩阵的各个池化区域的值，取平均后放在还原后的子矩阵位置；若采用Max池化，则把$\delta^l$的所有子矩阵的各个池化区域的值放在之前前向传播时最大值的位置，举例如下，$\delta^l$的第$k$个子矩阵为：
  
      $$
      \delta_k^l =
      \begin{bmatrix}
      2 & 8 \\\\
      4 & 6
      \end{bmatrix} \tag{16}
      $$
  
      > 已知池化大小为$2 * 2$，因此首先还原大小：
  
      $$
      \begin{bmatrix}
      0 & 0 & 0 & 0 \\\\
      0 & 2 & 8 & 0 \\\\
      0 & 4 & 6 & 0 \\\\
      0 & 0 & 0 & 0
      \end{bmatrix}
      \longrightarrow
      \begin{cases}
      \begin{bmatrix}
      2 & 0 & 0 & 0 \\\\
      0 & 0 & 0 & 8 \\\\
      0 & 4 & 0 & 0 \\\\
      0 & 0 & 6 & 0
      \end{bmatrix} & if \quad max \\\\
      \begin{bmatrix}
      0.5 & 0.5 & 2 & 2 \\\\
      0.5 & 0.5 & 2 & 2 \\\\
      1 & 1 & 1.5 & 1.5 \\\\
      1 & 1 & 1.5 & 1.5
      \end{bmatrix} & if \quad average
      \end{cases} \tag{17}
      $$
  
      > 因此，对于以上矩阵的反向传播可表示为:
  
      $$
      \delta_k^{l - 1} = \frac{\partial J(W, b)}{\partial a_k^{l - 1}} \frac{\partial a_k^{l - 1}}{\partial z_k^{l - 1}} = upsample(\delta_k^l) \odot \sigma'(z_k^{l - 1}) \tag{18}
      $$
  
      > 将以上扩展到张量层次，则：
  
      $$
      \delta^{l - 1} = upsample(\delta^{l}) \odot \sigma'(z^{l - 1}) \tag{19}
      $$
  
    + 已知卷积层$\delta^l$， 推到上一隐藏层$\delta^{l - 1}$
  
      > 类比DNN中的反向传播，CNN中形式类似，有部分改动，如下：
  
      $$
      \delta^{l - 1} = \delta^l rot180(W^l) \odot \sigma'(z^{l - 1}) \tag{20}
      $$
  
      > 其中，$W$进行了180旋转操作，以下举例该做法的原因，假设第$l - 1$层的输出$a^{l - 1}$是$3 * 3$矩阵，第$l$层卷积核$W^l$是$2 * 2$矩阵，假设$b^l$忽略不计，则有：
  
      $$
      z^l = a^{l - 1} * W^l \tag{21}
      $$
  
      > 即：
  
      $$
      \begin{bmatrix}
      a_{11} & a_{12} & a_{13} \\\\
      a_{21} & a_{22} & a_{23} \\\\
      a_{31} & a_{32} & a_{33}
      \end{bmatrix} * 
      \begin{bmatrix}
      w_{11} & w_{12} \\\\
      w_{21} & w_{22}
      \end{bmatrix} = 
      \begin{bmatrix}
      z_{11} & z_{12} \\\\
      z_{21} & z_{22}
      \end{bmatrix} \tag{22}
      $$
  
      > 反向梯度求导，有：
  
      $$
      \nabla a^{l - 1} = \frac{\partial J(W, b)}{\partial a^{l - 1}} = \frac{\partial J(W, b)}{\partial z^l} \frac{\partial z^l}{\partial a^{l - 1}} = \delta^l \frac{\partial z^l}{\partial a^{l - 1}} \tag{23}
      $$
  
      > 对每一项进行拆分，最后合并为矩阵卷积形式如下：
  
      $$
      \begin{bmatrix}
      0 & 0 & 0 & 0 \\\\
      0 & \delta_{11} & \delta_{12} & 0 \\\\
      0 & \delta_{21} & \delta_{22} & 0 \\\\
      0 & 0 & 0 & 0
      \end{bmatrix} * 
      \begin{bmatrix}
      w_{22} & w_{21} \\\\
      w_{12} & w_{11}
      \end{bmatrix} = 
      \begin{bmatrix}
      \nabla a_{11} & \nabla a_{12} & \nabla a_{13} \\\\
      \nabla a_{21} & \nabla a_{22} & \nabla a_{23} \\\\
      \nabla a_{31} & \nabla a_{32} & \nabla a_{33}
      \end{bmatrix} \tag{24}
      $$
  
    + 已知卷积层$\delta^l$，推导该层$W, b$梯度
  
      > 对于卷积层，有如下关系：
  
      $$
      z^l = a^{l - 1} * W^l + b^l \tag{25}
      $$
  
      > 因此对于$W$，：
  
      $$
      \frac{\partial J(W, b)}{\partial W^l} = \frac{\partial J(W, b)}{\partial z^l} \frac{\partial z^l}{\partial W^l} = \delta^l * a^{l - 1} \tag{26}
      $$
  
      > 对应每项，有：
  
      $$
      \frac{\partial J(W, b)}{\partial W_{pq}^l} = \sum_i \sum_j \delta_{ij}^l a_{i + p - 1, j + q - 1}^{l - 1} \tag{27}
      $$
  
      > 同理，对于$b$，有：
  
      $$
      \frac{\partial J(W, b)}{\partial b^l} = \frac{\partial J(W, b)}{\partial z^l} \frac{\partial z^l}{\partial b^l} = \sum_i \sum_j \delta_{ij} \tag{28}
      $$
  
  + 算法流程
  
    >初始化各隐藏层和输出层的$W, b$
  
    > 循环变量$\rightarrow$迭代次数：$epoch \in [1, epochs]$
    >
    > > 循环变量$\rightarrow$样本序号：$i \in [1, M]$
    > >
    > > > CNN模型输入对应格式的张量$a^1$
    > >
    > > > 循环变量$\rightarrow$网络层数：$l \in [2, L - 1]$
    > > >
    > > > > 若当前层为全连接层，则$a^{i, l} = \sigma(z^{i, l}) = \sigma(W^l a^{i, l - 1} + b^l)$
    > > >
    > > > > 若当前层为卷积层，则$a^{i, l} = \sigma(z^{i, l}) = \sigma(W^l * a^{i, l - 1} + b^l)$
    > > >
    > > > > 若当前层为池化层，则$a^{i, l} = pool(a^{i, l - 1})$
    > >
    > > > 当前层为输出层，则$a^{i, L} = softmax(z^{i, L}) = softmax(W^L a^{i, L - 1} + b^L)$
    > >
    > > > 根据选择的损失函数计算输出层误差$\delta^{i, L}$
    > >
    > > > 循环变量$\rightarrow$网络层数：$l \in [L - 1, 2]$
    > > >
    > > > > 若当前层为全连接层，则$\delta^{i, l} = (W^{l + 1})^T\delta^{i, l + 1} \odot \sigma'(z^{i, l})$
    > > >
    > > > > 若当前层为卷积层，则$\delta^{i, l} = \delta^{i, l + 1} * rot180(W^{l + 1}) \odot \sigma'(z^{i, l})$
    > > >
    > > > > 若当前层为池化层，则$\delta^{i, l} = upsample(\delta^{i, l + 1}) \odot \sigma'(z^{i, l})$
    >
    > > 循环变量$\rightarrow$网络层数：$l \in [2, L]$
    > >
    > > > 若当前层为全连接层，则$W^l = W^l - \alpha \sum_{i=1}^{M}\delta^{i, l}(a^{i, l - 1})^T, \quad b^l = b^l - \alpha \sum_{i = 1}^{M} \sum_p \delta_{p}^{i, l}$
    > >
    > > > 若当前层为卷积层，则$W^l = W^l - \alpha \sum_{i = 1}^{M} \delta^{i, l} * a^{i, l - 1}, \quad b^l = b^l - \alpha \sum_{i = 1}^{M} \sum_p \sum_q \delta_{pq}^{i, l}$
    >
    > > 若梯度变化量小于设定的停止迭代阈值$\epsilon$，则跳出循环
    >
    > 输出各隐藏层与输出层的线性关系系数矩阵$W$和偏置$b$
  
+ BatchNorm层

  > 参数标准化手段，网络收敛更快，一般在深度网络中采用卷积层-BN层-ReLU层-池化层作为标准单元块。

  + 前向传播

    > 训练阶段,首先计算当前batch的$\mu_B, {\sigma_B}^2$，其表示式如下：

    $$
    \mu_B = \frac{1}{m} \sum_{i = 1}^{m}x_i \tag{29}
    $$

    $$
    {\sigma_B}^2 = \frac{1}{m} \sum_{i = 1}^{m} (x_i - \mu_B)^2 \tag{30}
    $$
    
    > 则BN层输出可表示为：
    
    $$
    \tilde{x}_{train} = \frac{x_{train} - \mu_B}{\sqrt{{\sigma_B}^2 + \epsilon}} \cdot \gamma + \beta \tag{31}
    $$
    
    > 对于$\mu_B, {\sigma_B}^2$的全局取值更新如下：
    
    $$
    \begin{cases}
    \mu_r = \kappa \cdot \mu_r + (1 - \kappa) \cdot \mu_B \\\\
    {\sigma_r}^2 = \kappa \cdot {\sigma_r}^2 + (1 - \kappa) \cdot {\sigma_B}^2
    \end{cases} \tag{32}
    $$
    
    > 其中，$\gamma, \beta$根据梯度下降算法进行训练更新。在测试阶段，按照最终更新的上述参数进行代入计算如下：
    
    $$
    \tilde{x}_{test} = \frac{x_{test} - \mu_r}{\sqrt{{\sigma_r}^2 + \epsilon}} \cdot \gamma + \beta \tag{33}
    $$
    
  + TensorFlow实现
  
    ```python
    # network
    network = Sequential([
        layers.Conv2D(6, kernel_size=3, strides=1),
        layers.BatchNormalization(),
        layers.MaxPooling2D(pool_size=2, strides=2),
        layers.ReLU(),
        layers.Conv2D(16, kernel_size=3, strides=1),
        layers.BatchNormalization(),
        layers.MaxPooling2D(pool_size=2, strides=2),
        layers.ReLU(),
        layers.Flatten(),
        layers.Dense(120, activation='relu'),
        layers.Dense(84, activation='relu'),
        layeras.Dense(10)
    ])
    
    # training stage
    out = network(x, training=True)
    
    # testing stage
    out = network(x, training=False)
    ```
  
    

