##### 深度残差网络

+ 背景

  > 传统卷积神经网络随着层数深入，网络由于梯度弥散和梯度爆炸等现象越来越难以训练。

  > 添加回退机制，即模型本身复杂度太高可自动回退到低复杂度版本。

+ 原理

  > ResNet在卷积层的输入和输出之间添加Skip Connection实现层数回退机制。输入$x$通过两个卷积层，得到特征变换后的输出$F(x)$，与输入$x$进行对应元素的相加运算，得到最终输出$H(x)$，计算过程如下：

  $$
  H(x) = F(x) + x \tag{1}
  $$

  > $H(x)$称为残差模块，神经网络需要学习的目标变为：

  $$
  F(x) = H(x) - x \tag{2}
  $$

  > 观察上式，需要保证$F(x)$和$x$满足相加要求，即两者尺寸一致。一般可通过添加额额外的卷积运算环节将输入$x$变换到$F(x)$相同的尺寸，一般采用$1 \times 1$卷积运算，主要用于调整输入的通道数量。

+ 代码

  > 该部分代码见下一节“实战”。

##### DenseNet

+ 介绍

  > 借鉴ResNet中的Skip Connection思想，提出了DenseNet，该模型将前面所有层的特征图信息通过Skip Connection与当前层输出进行聚合；不同于前者，DenseNet采用在通道轴c维度进行拼接操作，聚合特征信息。