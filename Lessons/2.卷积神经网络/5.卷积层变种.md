##### 卷积层变种

+ 空洞卷积

  > 普通卷积层：减少网络参数量，卷积核通常设计为$1 \times 1$或者$3 \times 3$感受野大小。
  >
  > > 优点：参数量有效减少，加快训练
  >
  > > 缺点：网络特征提取的感受野区域有限
  >
  > 空洞卷积层：增加参数控制感受野区域的采样步长
  >
  > > 优点：在增加感受野的效果下不增加参数数量
  >
  > > 缺点：可能错失细节特征，需要精心设计步长避免出现网格效应

  ```python
  layer = layers.Conv2D(1, kernel_size=3, strides=1, dilation_rate=2)
  ```

+ 转置卷积

  > 通过在输入之间填充大量padding实现输出宽高大于输入宽高，实现向上采样的目标。

  > 以下讨论转置卷积的具体过程，其中各参数意义如下：

  $$
  \begin{matrix}
  h & \rightarrow & height \\\\
  w & \rightarrow & width \\\\
  i & \rightarrow & input\_size & (h = w = i) \\\\
  s & \rightarrow & strides \\\\
  p & \rightarrow & paddings \\\\
  o & \rightarrow & output\_size \\\\
  k & \rightarrow & kernel\_size
  \end{matrix} \tag{1}
  $$

  > 回顾普通卷积公式，有如下关系：

  $$
  o =
  \begin{bmatrix}
  \frac{i + 2 * p - k}{s'}
  \end{bmatrix} 
  + 1
  \tag{2}
  $$

  > 在转置卷积中，考虑$o + 2p - k = \tau s, \tau \in int$，则进行反向推导有：

  $$
  o = (i - 1)s + k - 2p \tag{3}
  $$

  > 需要注意，转置卷积并非真正意义上的反卷积，仅能够恢复出等大小的张量。

  ```python
  xx = tf.nn.conv2d_transpose(out, w, strides=2, padding='valid', output_shape=[1, 5, 5, 1])
  
  layer = layers.Conv2DTranspose(1, kernel_size=3, strides=1, paddong='valid')
  
  # if padding='valid': o = (i - 1) * s + k
  # if padding='same': o = i * s
  ```

  > 当以上倍数关系并不满足时，向下取整操作会使很多不同输入尺寸对应到相同的输出尺寸上，因此需要在函数中指定需要输出的张量尺寸。

  > 转置卷积具有“放大特征图”的功能，在生成对抗网络、语义分割中得到广泛应用。

+ 分离卷积

  > 通过对多通道特征的卷积计算，与普通卷积对比如下：
  >
  > > 输入：[1,  h, w, 3]
  >
  > > 普通卷积计算过程如下：

  $$
  [1, h, w, 3] \rightarrow \kappa = [3, 3, 3, 4] \rightarrow [1, h', w', 4] \tag{4}
  $$

  > > 参数量为：$3 \cdot 3 \cdot 3 \cdot 4 = 108$。
  >
  > > 分离卷积计算过程如下：

  $$
  [1, h, w, 3] \rightarrow \kappa_1 = [3, 3, 3, 1] \rightarrow \kappa_2 = [1, 1, 3, 4] \rightarrow = [1, h', w', 4] \tag{5}
  $$

  > > 参数量为：$3 \cdot 3 \cdot 3 \cdot 1 + 1 \cdot 1 \cdot 3 \cdot 4 = 39$。
  >
  > 因此，分离卷积通过两步卷积运算实现了运算量的降低，长应用于MobileNets等。