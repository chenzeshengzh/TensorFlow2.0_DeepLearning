##### 反向传播

+ 激活函数导数

  > 对于神经网络中常用的激活函数，本节给出其导数推导过程。
  >
  > + $Sigmoid$
  >   $$
  >   \sigma(x) = \frac{1}{1 + e^{-x}} \tag{1}
  >   $$
  >
  >   > 对以上函数进行求导如下：
  >
  >   $$
  >   \begin{aligned}
  >   \frac{d}{dx} \sigma(x)
  >   &= \frac{d}{dx}(\frac{1}{1 + e^{-x}}) = \frac{e^{-x}}{(1 + e^{-x})^2} \\\\
  >   &= \frac{1 + e^{-x} - 1}{(1 + e^{-x})^2} = \frac{1}{1 + e^{-x}} - (\frac{1}{1 + e^{-x}})^2 \\\\
  >   &= \sigma(1-\sigma)
  >   \end{aligned} \tag{2}
  >   $$
  >
  >   ```python
  >   import numpy as np
  >   def sigmoid(x):
  >       return 1 / (1 + np.exp(-x))
  >   
  >   def derivative(x):
  >       return sigmoid(x) * (1 - sigmoid(x))
  >   ```
  >
  >   ---
  >
  > + $ReLU$
  >   $$
  >   \operatorname{ReLU}(x) = \operatorname{max}(0, x) \tag{3}
  >   $$
  >
  >   > 对以上函数进行求导如下：
  >
  >   $$
  >   \frac{d}{dx}\operatorname{ReLU} =
  >   \begin{cases}
  >   1 & x \ge 0 \\\\
  >   0 & x < 0
  >   \end{cases} \tag{4}
  >   $$
  >
  >   ```python
  >   import numpy as np
  >   def relu(x):
  >       d = np.array(x, copy=True)
  >       d[x < 0] = 0
  >       return d
  >   
  >   def derivative(x):
  >       d = np.ones_like(x)
  >       d[x < 0] = 0
  >       return d
  >   ```
  >
  >   ---
  >
  > + $LeakyReLU$
  >   $$
  >   \operatorname{LeakyReLU}=
  >   \begin{cases}
  >   x & x \ge 0 \\\\
  >   px & x < 0
  >   \end{cases} \tag{5}
  >   $$
  >
  >   > 对以上函数进行求导如下：
  >
  >   $$
  >   \frac{d}{dx}\operatorname{LeakyReLU}=
  >   \begin{cases}
  >   1 & x \ge 0 \\\\
  >   p & p < 0
  >   \end{cases} \tag{6}
  >   $$
  >
  >   ```python
  >   import numpy as np
  >   def leaky_relu(x, p):
  >       d = np.array(x, copy=True)
  >       d[x < 0] = p * x
  >       return d
  >   
  >   def derivative(x, p):
  >       d = np.ones_like(x)
  >       d[x < 0] = p
  >       return d
  >   ```
  >
  >   ---
  >
  > + $Tanh$
  >   $$
  >   \operatorname{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2 \cdot \sigma(2x) - 1 \tag{7}
  >   $$
  >
  >   > 对以上函数进行求导如下：
  >
  >   $$
  >   \begin{aligned}\frac{d}{dx}\operatorname{tanh}(x)
  >   &= \frac{(e^x + e^{-x})(e^x + e^{-x}) - (e^x - e^{-x})(e^x - e^{-x})}{(e^x + e^{-x})^2} \\\\
  >   &= 1 - \operatorname{tanh}^2(x)
  >   \end{aligned} \tag{8}
  >   $$
  >
  >   ```python
  >   import numpy as np
  >   def sigmoid(x):
  >       return 1 / (1 + np.exp(-x))
  >   
  >   def tanh(x):
  >       return 2 * sigmoid(2 * x) - 1
  >   
  >   def derivative(x):
  >       return 1 - tanh(x) ** 2
  >   ```

+ 损失函数梯度

  >对于神经网络中常用的损失函数，本节给出其导数推导过程。
  >
  >+ 均方损失
  >
  >   $$
  >   L = \frac{1}{2} \sum_{k=1}^{K}(y_k - o_k)^2 \tag{9}
  >   $$
  >
  >   > 对以上函数进行求导如下：
  >
  >   $$
  >   \begin{aligned}
  >   \frac{\partial L}{\partial o_i}
  >   &=\frac{1}{2}\sum_{k=1}^{K}\frac{\partial}{\partial o_i}(y_k-o_k)^2 \\\\
  >   &= \frac{1}{2}\sum_{k=1}^{K}2 \cdot (y_k - o_k)\cdot\frac{\partial(y_k - o_k)}{\partial o_i} \\\\
  >   &= \sum_{k=1}^{K}(o_k - y_k) \cdot \frac{\partial o_k}{\partial o_i} = o_i - y_i
  >   \end{aligned}\tag{10}
  >   $$
  >
  >---
  >
  >+ 交叉熵损失
  >
  >   + $Softmax$梯度
  >
  >   $$
  >   p_i = \frac{e^{z_i}}{\sum_{k=1}^{K}e^{z_k}} \tag{11}
  >   $$
  >
  >   > 对以上函数进行求导如下：
  >
  >   $if \quad i = j:$
  >   $$
  >   \begin{aligned}
  >   \frac{\partial p_i}{\partial z_j}
  >   &= \frac{\partial \frac{e^{z_i}}{\sum_{k=1}^{K}e^{z_k}}}
  >   {\partial z_j} = \frac{e^{z_i}\sum_{k=1}^{K}e^{z_k} -
  >   e^{z_j}e^{z_i}}{(\sum_{k=1}^{K}e^{z_k})^2} \\\\
  >   &= \frac{e^{z_i}}{\sum_{k=1}^{K}e^{z_k}} \times
  >   \frac{\sum_{k=1}^{K}e^{z_k} - e^{z_j}}
  >   {\sum_{k=1}^{K}e^{z_k}} \\\\
  >   &= p_i(1-p_j)
  >   \end{aligned} \tag{12}
  >   $$
  >
  >   $if \quad i \ne j:$
  >   $$
  >   \begin{aligned}
  >   \frac{\partial p_i}{\partial z_j}
  >   &= \frac{\partial \frac{e^{z_i}}{\sum_{k=1}^{K}e^{z_k}}}
  >   {\partial z_j} = \frac{0 - e^{z_j}e^{z_i}}
  >   {(\sum_{k=1}^{K}e^{z_k})^2} \\\\
  >   &= -p_i \cdot p_j
  >   \end{aligned} \tag{13}
  >   $$
  >
  >   > 综上所述，分类层的梯度可以表示如下：
  >
  >   $$
  >   \frac{\partial p_i}{\partial z_j}=
  >   \begin{cases}
  >   p_i(1-p_j) & i=j \\\\
  >   -pi \cdot p_j & i \ne j
  >   \end{cases} \tag{14}
  >   $$
  >
  >   + 交叉熵梯度
  >
  >   $$
  >   L = -\sum_{k}y_k\operatorname{log}(p_k) \tag{15}
  >   $$
  > 
  >   > 对以上函数进行求导如下：
  >   
  >   $$
  >   \begin{aligned}
  >   \frac{\partial L}{\partial z_i}
  >   &= -\sum_k y_k \frac{\partial \operatorname{log}(p_k)}{\partial z_i} = -\sum_k y_k \frac{\partial \operatorname{log}(p_k)}{\partial p_k} \frac{\partial p_k}{\partial z_i} \\\\
  >   &= -\sum_k y_k \frac{1}{p_k}\frac{\partial p_k}{\partial z_i} \\\\
  >   &= -y_i \cdot \frac{1}{p_i} \cdot pi \cdot (1-p_i) + \sum_{k \ne i}y_k \cdot \frac{1}{p_k} \cdot p_k \cdot p_i \\\\
  >   &= y_ip_i - y_i + \sum_{k \ne i}y_kp_i = p_i - y_i
  >   \end{aligned} \tag{16}
  >   $$
  
+ 神经网络推导

  >+ 单神经元推导
  >
  >   > 假设单神经网络采用$sigmoid$激活函数，即：
  >
  >   $$
  >   o^{(1)} = \sigma({w^{(1)}}^Tx + b^{(1)}) \tag{17}
  >   $$
  >
  >   > 若考虑采用均方损失函数，即：
  > 
  >   $$
  >   L = \frac{1}{2}(o_1^{(1)} - t)^2=\frac{1}{2}(o_1 - t)^2 \tag{18}
  >   $$
  >
  >   > 考虑对权值变量$w_{j1}$的导数，如下：
  > 
  >   $$
  >   \begin{aligned}
  >   \frac{\partial L}{\partial w_{j1}}
  >   &= (o_1 - t) \cdot \frac{\partial o_1}{\partial w_{j1}} = (o_1 - t) \cdot \frac{\partial \sigma(z_1)}{\partial w_{j1}} \\\\
  >   &= (o_1 - t) \cdot \sigma(z_1) \cdot (1 - \sigma(z_1)) \cdot \frac{\partial z_1^{(1)}}{\partial w_{j1}} \\\\
  >   &= (o_1 - t)o_1(1 - o_1)x_j
  >   \end{aligned} \tag{19}
  >   $$
  > + 全连接层梯度
  >
  >   > 将以上单个神经元模型迁移到单层的全连接网络上，均方误差可表示如下：
  >   $$
  >   L = \frac{1}{2}\sum_{i=1}^{K}(o_i^{(1)} - t_i)^2 \tag{20}
  >   $$
  >   > 考虑对权重参数的梯度如下：
  >   $$
  >   \begin{aligned}
  >   \frac{\partial L}{\partial w_{jk}}
  >   &= (o_k - t_k) \frac{\partial o_k}{\partial w_{jk}} = (o_k - t_k) \frac{\partial \sigma(z_k)}{\partial w_{jk}} \\\\
  >   &= (o_k - t_k)o_k(1-o_k)x_j
  >   \end{aligned} \tag{21}
  >   $$
  >   > 令$\delta_k=(o_k - t_k)o_k(1 - o_k)$，则上式可表示为：
  >   $$
  >   \frac{\partial L}{\partial w_{jk}} = \delta_k x_j  \tag{22}
  >   $$
  >   > 用如上表示方法，则梯度与起始节点$x_j$和终止节点$\delta_k$有关。
  > 
  >+ 层间传递
  > 
  >   > 输出层的梯度反向传播可参照式(21)，考虑倒数第二层的偏导数$\frac{\partial L}{\partial w_{ij}}$，输出层节点数为$K$,倒数第二层节点数为$J$，倒数第三层节点数为$I$，部分变量上标省略，均方损失为：
  >
  >   $$
  >   \begin{aligned}
  >   \frac{\partial L}{\partial w_{ij}}
  >   &= \frac{\partial}{\partial w_{ij}}\frac{1}{2}\sum_k(o_k - t_k)^2 = \sum_k(o_k - t_k)\frac{\partial}{\partial w_{ij}}\sigma(z_k) \\\\
  >   &= \sum_k(o_k - t_k)o_k(1 - o_k)\frac{\partial z_k}{\partial w_{ij}} \\\\
  >   &= \sum_k(o_k - t_k)o_k(1 - o_k)\frac{\partial z_k}{\partial o_j}\frac{\partial o_j}{\partial w_{ij}} \\\\
  >   &= \sum_k(o_k - t_k)o_k(1 - o_k)w_{jk}\frac{\partial o_j}{\partial w_{ij}} \\\\
  >   &= o_j(1 - o_j)\frac{\partial z_j}{\partial w_{ij}}\sum_K(o_k - t_k)o_k(1 - o_k)w_{jk} \\\\
  >   &= o_j(1 - o_j)o_i \sum_k \underbrace{(o_k - t_k)o_k(1 - o_k)}\_{\delta_k^K} w_{jk} \\\\
  >   &= \underbrace{o_j(1 - o_j) \sum_k \delta_k^{K}w_{jk}}\_{\delta_j^J}o_i
  >   \end{aligned} \tag{23}
  >   $$
  >   > 根据以上推导，可得出以下规律：
  > 
  >   $输出层：$
  >   $$
  >   \frac{\partial L}{\partial w_{jk}} = \delta_k^Ko_j \tag{24}
  >   $$
  > 
  >   $$
  >   \delta_k^K = o_k(1 - o_k)(o_k - t_k) \tag{25}
  >   $$
  >   $倒数第二层：$
  >   $$
  >   \frac{\partial L}{\partial w_{ij}} = \delta_j^Jo_i \tag{26}
  >   $$
  > 
  >   $$
  >   \delta_j^J = o_j(1 - o_j)\sum_k\delta_k^Kw_{jk} \tag{27}
  >   $$
  >   $倒数第三层：$
  >   $$
  >   \frac{\partial L}{\partial w_{ni}} = \delta_i^I \tag{28}
  >   $$
  > 
  >   $$
  >   \delta_i^I = o_i(1 - o_i)\sum_j\delta_j^Jw_{ij} \tag{29}
  >   $$
  > 

