#### 简单神经网络

+ 全连接层

  > 由于感知机不可导特性，无法解决复杂问题，通过更换感知机的激活函数，同时并行堆叠多个神经元实现多输入、多输出的网络层结构。以3输入节点、2输出节点为例：
  >
  > 第一个输出节点的输出为：
  > $$
  > o_1 = \sigma(w_{11}\cdot x_1+w_{21}\cdot x_2+w_{31}\cdot x_3+b_1) \tag{1}
  > $$
  > 第二个输出节点的输出为：
  > $$
  > o_2 = \sigma(w_{12} \cdot x_1 + w_{22} \cdot x_2 + w_{32} \cdot x_3 + b_2) \tag{2}
  > $$
  > 输出向量为$o=[o_1, o_2]$，整个网络层可以通过矩阵关系式表达：
  > $$
  > \begin{bmatrix}
  > o_1 & o_2
  > \end{bmatrix} = 
  > \begin{bmatrix}
  > x_1 & x_2 & x_3
  > \end{bmatrix} @ 
  > \begin{bmatrix}
  > w_{11} & w_{12} \\\\
  > w_{21} & w_{22} \\\\
  > w_{31} & w_{32}
  > \end{bmatrix} + 
  > \begin{bmatrix}
  > b_1 & b_2
  > \end{bmatrix} \tag{3}
  > $$
  > 即写成矩阵相乘形式如下：
  > $$
  > \boldsymbol {O = X @ W + b} \tag{4}
  > $$
  > 输入矩阵$\boldsymbol{X \in R^{b, d_{in}}}$ ，$b$为样本数量，$d_{in}$为输入节点数；权值矩阵$\boldsymbol{W \in R^{d_{in}, d_{out}}}$，$d_{out}$为输出节点数；偏置向量$\boldsymbol{b \in R^{d_{out}}}$。
  >
  > 考虑多个样本并行计算，以两个输入样本为例，可将(3)式推广到批量形式：
  > $$
  > \begin{bmatrix}
  > o_1^{(1)} & o_2^{(1)} \\\\
  > o_1^{(2)} & o_2^{(2)}
  > \end{bmatrix} = 
  > \begin{bmatrix}
  > x_1^{(1)} & x_2^{(1)} & x_3^{(1)} \\\\
  > x_1^{(2)} & x_2^{(2)} & x_3^{(2)}
  > \end{bmatrix} @
  > \begin{bmatrix}
  > w_{11} & w_{12} \\\\
  > w_{21} & w_{22} \\\\
  > w_{31} & w_{32}
  > \end{bmatrix} +
  > \begin{bmatrix}
  > b_1 & b_2
  > \end{bmatrix} \tag{5}
  > $$
  > 该网络层称为全连接网络层或稠密连接层。

+ 优化目标

  > 在如上的全连接层网络中，前向传播的最后一步完成误差计算如下：
  > $$
  > L = g(f_{\theta}(x), y) \tag{6}
  > $$
  > 其中，$f_{\theta}(\cdot)$代表参数为$\theta$的神经网络模型，$g(\cdot)$代表误差函数，用来描述当前网络预测值$f_{\theta}(x)$与真实标签$y$之间的差距度量，常见的损失函数有均方损失或交叉熵损失。$L$称为网络误差，因此网络模型的优化问题变为在训练集$D^{train}$上学习到一组参数$\theta$使得训练误差$L$最小，即：
  > $$
  > \theta^{\*} = \underset{\theta}{\operatorname{arg \, min}} \, g(f_{\theta}(x), y), x \in D^{train} \tag{7}
  > $$
  > 上述最优化问题利用误差反向传播算法求解，利用梯度下降算法迭代更新参数如下：
  > $$
  > \theta' = \theta - \eta \cdot \nabla_{\theta}L \tag{8}
  > $$
  > 其中$\eta$为学习率。

+ 激活函数

  > 上文中的网络输出为线性计算过程，即不包含激活函数，当处理非线性等复杂问题时，需要对线性处理的结果添加激活函数，本节讨论常用的激活函数。
  >
  > + $Sigmoid$
  >   $$
  >   {\operatorname{Sigmoid}}(x) = \frac{1}{1 + e^{-x}} \tag{9}
  >   $$
  >
  >   > 概率分布(0,1)，可通过该函数实现概率输出
  >
  >   > 可理解为某中信号强度，如像素颜色经过归一化后，1表示该通道颜色浓度最高，即为原始255色值，0代表浓度最低，即为原始0色值；在门控系统中0表示全部关闭，1表示全部开放。
  >
  >   ```python
  >   tf.nn.sigmoid(x)
  >   ```
  >
  > + $ReLU$
  >   $$
  >   \operatorname{ReLU}(x) = \operatorname{max}(0, x) \tag{10}
  >   $$
  >
  >   > 小于0的值全部抑制为0，大于0的值返回其本身。
  >
  >   > 单侧抑制，相对宽松的兴奋边界，有效避免梯度弥散现象。
  >
  >   ```python
  >   tf.nn.relu(x)
  >   ```
  >
  > + $LeakyReLU$
  >   $$
  >   \operatorname{LeakyReLU} = 
  >   \begin{cases}
  >   x & x \ge 0 \\\\
  >   px & x < 0
  >   \end{cases} \tag{11}
  >   $$
  >
  >   > $p$ 为用户自行设置的某较小数值，使当$x<0$时也能够获得较小的导数，避免梯度弥散现象。
  >
  >   > $p$一般设置为小于1的数字，当$p=0$时，退化为ReLU函数。
  >
  >   ```python
  >   tf.nn.leaky_relu(x)
  >   ```
  >
  > + $Tanh$
  >
  >   $$
  >   \operatorname{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{_-x}} \tag{12}
  >   $$
  >   
  >   > 可通过$Sigmoid$函数缩放平移后得到，值域范围为[-1, 1]。
  >   
  >   > 一般用来反映数据的分布情况，在0左右处于对称分布（标准正太分布）
  >   
  >   ```python
  >   tf.nn.tanh(x)
  >   ```
  
+ 误差计算

  > + 均方误差
  >
  >   $$
  >   \operatorname{MSE}(\boldsymbol{y, o}) = \frac{1}{d_{out}}    \sum_{i=1}^{d_{out}}(y_i - o_i)^2 \tag{13}
  >   $$
  >
  >   > 将输出向量和真实向量映射到笛卡尔坐标系两个点上，计算该两点的欧式距离（平方）作为衡量两个向量之间的差距。
  >
  >   > 广泛应用于回归问题中，梯度平滑。
  >
  >   ```python
  >   # function
  >   loss = keras.losses.MSE(y_onehot, o)
  >
  >   # class
  >   criteon = keras.losses.MeanSquareError()
  >   ```
  >
  >+ 交叉熵损失
  >
  >  + 熵
  >    $$
  >    H(P) = - \sum_i{P(i) \operatorname{log_2}P(i)} \tag{14}
  >    $$
  >
  >    >熵，用来衡量信息的不确定度；不确定性越大，信息量越大。以上为分布$P(i)$的熵定义式。
  >
  >    > 熵，可采用其他底数的$\operatorname{log}$来计算，且熵总大于0。
  >
  >  + 交叉熵
  >    $$
  >    H(p||q) = - \sum_i{p(i) \operatorname{log_2}q(i)} \tag{15}
  >    $$
  >
  >    > 式(15)为分布$p$与分布$q$的交叉熵表达式。
  >
  >  + $KL$散度
  >    $$
  >    D_{KL}(p||q) = \sum_i{p(i)log(\frac{p(i)}{q(i)})} \tag{16}
  >    $$
  >
  >    > 式(16)为分布$p$和分布$q$的$KL$散度，注意其不具有对称关系。该指标用于衡量不同分布之间的距离。当分布相同时，取得最小值0；当分布差异越大，则该指标越大。
  >
  >  + 熵与$KL$散度
  >    $$
  >    \begin{aligned}
  >    H(p||q) 
  >    &= - \sum_i{p(i) \operatorname{log_2}q(i)} = \sum_i{p(i) \operatorname{log_2}\frac{1}{q(i)}} \\\\
  >    &= \sum_i{p(i) \operatorname{log_2}\frac{1}{p(i)} \cdot \frac{p(i)}{q(i)}} \\\\
  >    &= \sum_i{p(i) \operatorname{log_2}{\frac{1}{p(i)}}} + \sum_i{p(i) \operatorname{log_2}{\frac{p(i)}{q(i)}}} \\\\
  >    &= H(p) + D_{KL}(p||q)
  >    \end{aligned} \tag{17}
  >    $$
  >
  >    > 同样的，熵与$KL$散度均具有不对称性。
  >
  >  + 分类问题交叉熵
  >    $$
  >    \begin{aligned}
  >    H(p||q) 
  >    &= D_{KL}(p||q) = \sum_j{y_j \operatorname{log}(\frac{y_j}{o_j})} \\\\
  >    &= 1 \cdot \operatorname{log} \frac{1}{o_j} + \sum_{j \neq i}0 \cdot \operatorname{log}(\frac{0}{o_j}) = -\operatorname{log}o_j
  >    \end{aligned} \tag{18}
  >    $$
  >
  >    > 其中，$p$为原始分布，$q$为预测分布；分别对用$one-hot$编码下的标签$y$和预测$o$，且$H(p) = 0$特性。
