##### 循环神经网络先验知识

+ 序列表示

  > 文字编码为数值的过程称为Word Embedding。One-Hot编码不仅维度高而稀疏，且无法反映语言之间的语义相关性。

  > 衡量语义相关性，可使用余弦相似度，其定义如下：

  $$
  \operatorname{similarity}(\boldsymbol{a}, \boldsymbol{b}) = \cos(\theta) = \frac{\boldsymbol{a} \cdot \boldsymbol{b}}{|\boldsymbol{a}| \cdot |\boldsymbol{b}|} \tag{1}
  $$

+ Embedding层

  > 在神经网络中，单词的表示向量可以直接通过训练的方式得到，该训练层称为Embedding层，负责把单词编码为某个词向量$v$，计算过程如下：

  $$
  v = f_{\theta}(i|N_{vocab}, n) \tag{2}
  $$

  > 其中：$N_{vacab}$表示系统总单词数量，$n$表示输出向量的长度，$i$表示待编码的单词编号。

  > 构建Embedding层，需要构建一个尺寸为$[N_{vocab}, n]$的查询表对象table，对于任意的单词编号$i$，只需要查询到对应位置上的向量并进行返回，即：

  $$
  v = table[i] \tag{3}
  $$

  > 该层可放置在神经网络之前，完成单词到向量的转换，得到的表示向量可以继续通过神经网络完成后续任务，并计算误差$L$，采用梯度下降算法实现训练。

  ```python
  x = tf.range(10)
  x = tf.random.shuffle(x)
  net = layer.Embedding(10, 4)
  out = net(x)
  ```

  > 以上结果为随机初始化的$10 \times 4$对应矩阵，需要经过进一步训练，可以通过梯度下降算法进行优化。

+ Embedding表示

  + CBOW：给定目标单词上下文信息，预测该单词

    > 输入层：目标单词上下文单词序列$X=[x_{1k}, x_{2k}, \cdots, x_{Ck}]$，假设单词$One-Hot$维度为$V$，上下文单词数为$C$。

    > 隐藏层：权重矩阵$W$，其大小为$V \times N$，其中$N$为人工指定的目标向量维度。单词序列$X$与权重矩阵$W$相乘，并取序列中每个单词相乘后的结果作平均，作为该隐藏层的计算结果，维度为$1 \times N$。

    > 输出层：隐藏层结果与输出层权重向量$W'$相乘，其中$W'$尺寸为$N\times V$，因此输出层结果被还原到$One-Hot$编码的维度，其与真实标签进行比较，运用交叉熵损失函数进行优化即可。

  + Skip-Gram：给定目标单词，预测上下文信息

    > "The quick brown fox jumps over lazy dog"

    > skip_window=2

    > The $\rightarrow$(tthe, quick), (the, brown)
    >
    > quick$\rightarrow$(quick, the), (quick, brown), (quick, fox)
    >
    > brown$\rightarrow$(brown, the), (brown, quick), (brown, fox), (brown, jumps)
    >
    > $\cdots$

+ 预训练词向量

  > 目前应用广泛的预训练模型包含Word2Vec和GloVe，可以直接导出学习到的词向量表，方便迁移到其他任务。例如，GloVe.6B.60d模型中，每个单词采用长度为50的向量进行表示。此时，在搭建Embedding层时，不再使用随机初始化方式，而是载入预训练菜蔬初始化查询表。

  ```python
  embed_glove = load_embed('glove.6B.50d.txt')
  net.set_weights([embed_glove])
  # net.trainable = False
  ```

  