##### 循环神经网络

+ 问题

  > 在处理序列问题时，全连接网络具有以下缺陷：
  >
  > + 网络参数多，内存占用和计算代价高；序列长度变化，网络结构动态变化
  > + 每个子网络仅能感受到当前词向量输入，无法感知上下文信息，导致语义缺失

+ 前向传播

  > 在每个时间戳$t$上，网络层接受当前时间戳的输入$x_t$和上一个时间戳的网络状态向量$h_{t - 1}$，经过如下变换：

  $$
  h_t = f_{\theta}(h_{t - 1}, x_t) \tag{1}
  $$

  > 得到新状态向量$h_t$，并写入到内存中，其中$f_{\theta}$表示子网络运算逻辑。在每个时间戳上，网络层均产生输出$o_t$，其中：

  $$
  o_t = g_{\phi}(h_t) \tag{2}
  $$

  > 上述网络在时间戳上折叠，网络循环接受序列的每个特征向量$x_t$，并刷新内部状态向量$h_5$，同时形成输出$o_t$。对于这种网络结构，称为循环神经网络。将(1)式展开，有：

  $$
  h_t = \sigma(W_{xh}x_t + W_{hh}h_{t - 1} + b) \tag{3}
  $$

  > 对于输出，可直接取状态向量，即$o_t = h_t$；或者添加线性变换，即$o_t = W_{ho}h_t$。

+ 梯度传播

  > 由于$W_{hh}$在时间戳上共享，因此需要计算梯度求和如下：

  $$
  \frac{\partial L}{\partial W_{hh}} 
  = \sum_{i = 1}^{t}\frac{\partial L}{\partial o_t}\frac{\partial o_t}{\partial h_t}\frac{\partial h_t}{\partial h_i}\frac{\partial^+ h_i}{\partial W_{hh}} \tag{4}
  $$

  > 假设$o_t = h_t$，则有：

  $$
  \frac{\partial o_t}{\partial h_t} = I \tag{5}
  $$

  > 其中，$\frac{\partial^+ h_i}{\partial W_{hh}}$为只考虑一个时间戳的梯度传播，称为直接偏导数。

  $$
  \frac{\partial h_t}{\partial h_i} = \prod_{k=i}^{t-1}\frac{\partial h_{k+1}}{\partial h_k} \tag{6}
  $$

  > 考虑到前向传播公式，即：

  $$
  h_{k+1} = \sigma(W_{xh}x_{k+1} + W_{hh}h_k + b) \tag{7}
  $$

  > 则，公式(6)的分数项可计算为：

  $$
  \frac{\partial h_{k+1}}{\partial h_k} = W_{hh}^Tdiag(h'_{k+1}) \tag{8}
  $$

  > 因此：

  $$
  \frac{\partial h_t}{\partial h_i} = \prod_{k=i}^{t-1}W_{hh}^Tdiag(\sigma'(W_{xh}x_{k+1}+W_{hh}h_k+b)) \tag{9}
  $$

  > 由以上各式，可求得$\frac{\partial L}{\partial W_{hh}}$，同理，$\frac{\partial L}{\partial W_{xh}}$也可求得。

+ TensorFlow实现

  + SimpleRNNCell

    > 仅完成一个时间戳的前向运算，使用较少

    ```python
    h0 = [tf.zeros([4, 64])]
    x = tf.random.normal([4, 80, 100])
    xt = x[:, 0, :]
    cell = layers.SimpleRNNCell(64)
    out, h1 = cell(xt, h0)
    
    # forward propogation
    for xt in tf.unstack(x, axis=1):
        out, h = cell(xt, h)
    ```

    > 多层叠加使用时，可以提升网络的表达能力；另一方面，深层循环神经网络训练起来十分困难，因此一般控制在10层以内。

    ```python
    x = tf.random.normal([4, 80, 100])
    cell0 = layers.SimpleRNNCell(64)
    cell1 = layers.SimpleRNNCell(64)
    h0 = [tf.zeros([4, 64])]
    h1 = [tf.zeros([4, 64])]
    for xt in tf.unstack(x, axis=1):
        out0, h0 = cell0(xt, h0)
        out1, h1 = cell1(out0, h1)
        
    # another method
    middle_sequence = []
    for xt in tf.unstack(x, axis=1):
        out0, h0 = cell0(xt, h0)
        middle_sequence.append(out0)
    for xt in middle_sequence:
        out1, h1 = cell1(xt, h1)
    ```

  + SimpleRNN

    > 若不希望手动参与循环神经网络的计算过程，可使用该接口。

    ```python
    layer = layers.SimpleRNN(64, return_sequence=True)
    out = layer(x)
    
    # multi layers
    net = keras.Sequential([
        layers.SimpleRNN(64, return_sequence=True),
        layers.SimpleRNN(64)
    ])
    out = net(x)
    ```

    