##### 长短期记忆网络

+ 问题

  > 回忆RNN网络中梯度推导的重要表达式，如下：

  $$
  \frac{\partial h_t}{\partial h_i} = \prod_{j=i}^{t-1}W_{hh}^Tdiag(\sigma'(W_{xh}x_{j+1}+W_{hh}h_j+b)) \tag{1}
  $$

  > 可以发现，出现了$W_{hh}$连乘运算，导致当其特征值大于1时出现梯度爆炸，小于1时出现梯度弥散现象。

  > 梯度爆炸可以通过梯度裁剪在一定程度上进行解决，一般有如下三种方式:

  ```python
  # clip value
  a = tf.random.uniform([2, 2])
  b = tf.clip_by_value(a, 0.4, 0.6)
  
  # clip norm
  a = tf.random.uniform([2, 2])
  b = tf.clip_by_norm(a, 5)
  
  # clip global norm
  w1 = tf.random.normal([3, 3])
  w2 = tf.random.normal([3, 3])
  # global_norm = tf.math.sqrt(tf.norm(w1)**2 + tf.norm(w2)**2)
  (ww1, ww2), global_norm = tf.clip_by_global_norm([w1, w2], 2)
  ```

  > 梯度弥散，一般可以通过增大学习率、减少网络深度、添加Skip Connection等方式进行抑制。

+ LSTM

  + 原理

    > 引入门控策略，通过门控单元控制信息的遗忘和刷新。LSTM中存在两个状态向量：$c$为内部状态向量，可理解为内存状态向量记忆，$h$表示输出向量。该网络主要使用输入门、遗忘门和输出门控制内部信息的流动。

    > 遗忘门

    $$
    g_f = \sigma(W_f[h_{t-1},x_t]+b_f) \tag{1}
    $$

    > 经过遗忘没门后，状态向量变为$g_fc_{t-1}$。

    > 输入门

    > 首先对当前时间戳输入$x_i$和上一个时间戳输出$h_{t-1}$做非线性变换得到新的输入向量：

    $$
    \tilde{c}_t = \tanh(W_c[h_{t-1}, x_t]+b_c) \tag{2}
    $$

    > 输入门的控制变量，类比遗忘门可以写成：

    $$
    g_i = \sigma(W_i[h_{t-1}, x_t]+b_i) \tag{3}
    $$

    > 经过输入门后，待写入记忆的向量变为$g_i\tilde{c}_t$。

    > 刷新记忆，获得新的状态向量如下：

    $$
    c_t = g_i\tilde{c}_t + g_fc_{t-1} \tag{4}
    $$

    > 输出门

    > 类比输入门和遗忘门，输出门概率定义为：

    $$
    g_o = \sigma(W_o[h_{t-1},x_t]+b_o) \tag{5}
    $$

    > 则，最终输出的部分变为：

    $$
    h_t = g_o \cdot \tanh(c_t) \tag{6}
    $$

  + TensorFlow

    > LSTMCell

    > 用法类似于SimpleRNNCell，区别在于LSTM状态变量List有两个，为$[h_t, c_t]$，需要分别初始化。

    ```python
    x = tf.random.normal([2, 80, 100])
    cell = layers.LSTMCell(64)
    state = [tf.zeros([2, 64]), tf.zeros([2, 64])]
    for xt in tf.unstack(x, axis=1):
        out, state = cell(xt, state)
    ```

    > LSTM

    > 用法类似于SimpleRNN，可以一次完成整个序列的运算。

    ```python
    x = tf.random.normal([2, 80, 100])
    net = keras.Sequential([
        layers.LSTM(64, return_sequences=True),
        layer.LSTM(64)
    ])
    out = net(x)
    ```

+ GRU

  > 简化LSTM结构，保留遗忘门，将内部状态向量和输出向量合并，统一为状态向量$h$，门控包含：复位门和更新门。

  + 复位门

    > 用于控制上一个时间戳的状态$h_{t-1}$进入GRU的量，其关系式为：

    $$
    g_r = \sigma(W_r[h_{t-1}x_t]+b_r) \tag{7}
    $$

    > 门控向量$g_r$仅控制状态$h_{t-1}$，不控制输入$x_t$，新的中间状态为：

    $$
    \tilde{h}_t = \tanh(W_h[g_rh_{t-1},x_t]+b_h) \tag{8}
    $$

  + 更新门

    > 用于控制上一时间戳的状态$h_{t-1}$和新输入$\tilde{h}_t$对新状态向量$h_t$的影响，更新门控由下式取得：

    $$
    g_z = \sigma(W_z[h_{t-1},x_t]+b_z) \tag{9}
    $$

    > 则新的状态向量可以表示为：

    $$
    h_t = (1-g_z)h_{t-1} + g_z\tilde{h}_t \tag{10}
    $$

  + TensorFlow

    > 使用方法与LSTM类似，如下：

    ```python
    # cell
    x = tf.random.normal([2, 80, 100])
    h = [tf.zeros([2, 64])]
    cell = layers.GRUCell(64)
    for xt in tf.unstack(x, axis=1):
        out, h = cell(xt, h)
    
    # layer
    x = tf.random.normal([2, 80, 100])
    net = Sequential([
        layers.GRU(64, return_sequences=True),
        laeyrs.GRU(64)
    ])
    out = net(x)
    ```

    