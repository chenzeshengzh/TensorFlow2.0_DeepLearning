##### 过拟合

+ 基本概念

  > 欠拟合：训练集和测试集误差均较大，模型无法表达复杂的真实模型。

  > 过拟合：训练集误差较小，测试集误差较大，模型过分拟合训练样本，泛化能力差。

+ 预防过拟合

  > 提前停止：Early Stopping

  > 模型设计：若过拟合则减小神经网络层数或减小神经元个数

  > 正则化：添加惩罚项，简化神经网络

  > Dropout：随机丢弃神经元连接，bagging思想

  > 数据增强：通过收集或制作更多训练数据，提高模型的泛化能力。

+ 正则化

  + 基本概念

    > 以如下多项式函数模型为例：

    $$
    y = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \cdots + \beta_nx^n + \varepsilon \tag{1}
    $$

    > 上述模型中的网络参数$\beta_{k+1},\cdots,\beta_n$若为0，则网络实际容量退化到$k$次多项式的函数容量。因此，可以通过限制网络参数的稀疏性，约束网络的实际容量。根据此规则，在损失函数上添加额外的参数稀疏性惩罚项实现，未加惩罚项前的损失函数如下：
    
    $$
    \operatorname{min} L(f_{\theta}(x), y), (x, y)\in \boldsymbol{D}^{train} \tag{2}
    $$
    
    > 对模型参数添加额外约束后，优化目标变为：
    
    $$
    \operatorname{min}L(f_{\theta}(x), y) + \lambda \cdot \Omega(\theta),(x, y) \in \boldsymbol{D}^{train} \tag{3}
    $$
    
    > 其中，$\Omega(\theta)$表示对网络参数$\theta$的稀疏性约束。一般，可通过约束参数$\theta$的$L$范数实现，即：
    
    $$
    \Omega(\theta) = \sum_{\theta_i}||\theta_i||_l \tag{4}
    $$
    
    > 常用的正则化方式有$L0,L1,L2$正则化。
    
    ---
    
  + $L0$正则化
  
    > 采用$L0$范数作为稀疏性惩罚项$\Omega(\theta)$的正则化计算方式，即：
  
    $$
    \Omega(\theta) = \sum_{\theta_i}||\theta_i||_0 \tag{5}
    $$
  
    > 上式中，$||\theta_i||_0$定义为$\theta_i$中非零元素的个数，通过该约束可以使网络中的连接权值大部分为0，从而降低网络的实际参数量和网络容量；另一方面，该函数不可导，因此不能利用梯度下降算法进行优化，在神经网络中很少用到。
  
    ---
  
  + $L1$正则化
  
    > 采用$L1$范数作为稀疏性惩罚项$\Omega(\theta)$的正则化计算方式，即：
  
    $$
    \Omega(\theta) = \sum_{\theta_i}||\theta_i||_1 \tag{6}
    $$
  
    > 上式中，$||\theta_i||_1$定义为张量$\theta_i$中所有元素的绝对值之和；该函数可导，在神经网络中使用广泛。
  
    ```python
    loss_req = tf.reduce_sum(tf.math.abs(w1)) + tf.reduce_sum(tf.math.abs(w2))
    ```
  
    ---
  
  + $L2$正则化
  
    > 采用$L2$范数作为稀疏性惩罚项$\Omega(\theta)$的正则化计算方式，即：
    
    $$
    \Omega(\theta) = \sum_{\theta_i}||\theta_i||_2 \tag{7}
    $$
    
    > 上式中，$||\theta_i||$定义为张量$\theta_i$中所有元素的平方和；该函数连续可导，在神经网络中应用广泛。
    
    ```python
    loss_req = tf.reduce_sum(tf.square(w1) + tf.reduce_sum(tf.square(w2)))
    ```
  
+ Dropout

  > 训练时，随机断开神经网络的连接，减少实际参与计算的模型的参数量；测试时，恢复所有的连接，保证模型测试时获得最好的性能。

  ```python
  x = tf.nn.dropout(x, rate=0.5)
  
  model.add(layers.Dropout(rate=0.5))
  ```

+ 数据增强

  > 增加数据规模是解决过拟合最重要的途径，以图片为例，包含的数据增强方法包括：旋转、缩放、平移、裁剪、改变视角、遮挡局部等多种方式。

  + 旋转

    > 旋转获取不同角度的新图片，图片标签信息维持不变。

    ```python
    x = tf.image.rot90(x, 2)
    ```

    ---

  + 翻转

    > 水平翻转或竖直翻转

    ```python
    x = tf.image.random_flip_left_right(x)
    
    x = tf.image.random_flip_up_down(x)
    ```

    ---

  + 裁剪

    > 原图的左右或上下方向去掉部分边缘像素，保持图片主体不变；在实际操作时，一般先将图片缩放到略大于网络输入尺寸的大小，再裁切到合适大小。

    ```python
    x = tf.image.resize(x, [244, 244])
    x = tf.image.random_crop(x, [224, 224, 3])
    ```

    ---

  + 生成

    > 通过生成模型在原有数据上进行训练，学习到真实数据的分布，从而利用生成模型得到新的样本，如GAN，生成对抗网络。

    ---

  + 其他

    > 添加高斯噪声、变换视角、随机擦除等。
